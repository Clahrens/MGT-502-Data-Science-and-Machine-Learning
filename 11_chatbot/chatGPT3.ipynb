{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF0oZsWyr6pl",
        "outputId": "204ee4a8-c351-4899-9b8e-ce1f949bb42b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\charlotte ahrens\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\charlotte ahrens\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\charlotte ahrens\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\charlotte ahrens\\anaconda3\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "wtt846gbsFKf",
        "outputId": "e62419da-7892-4fb0-f394-a6329e8ab332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 414, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1323, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1051, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Charlotte Ahrens\\AppData\\Local\\Temp\\ipykernel_4188\\3236740603.py\", line 10, in CustomChatGPT\n",
            "    response = openai.ChatCompletion.create(\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
            "    return super().create(*args, **kwargs)\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
            "    response, _, api_key = requestor.request(\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
            "    resp, got_stream = self._interpret_response(result, stream)\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
            "    self._interpret_response_line(\n",
            "  File \"c:\\Users\\Charlotte Ahrens\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
            "    raise self.handle_error_response(\n",
            "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cd001e2adc5e0fec36d5dc709e0a979b in your message.)\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import gradio\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are the director of the Master's program in Information Systems at the University of Lausanne\"}]\n",
        "\n",
        "def CustomChatGPT(user_input): \n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model = \"gpt-3.5-turbo\",\n",
        "        messages = messages\n",
        "    )\n",
        "    ChatGPT_reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    messages.append({\"role\": \"assistant\", \"content\": ChatGPT_reply})\n",
        "    return ChatGPT_reply\n",
        "\n",
        "demo = gradio.Interface(fn=CustomChatGPT, inputs = \"text\", outputs = \"text\", title = \"Master's Director\")\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
