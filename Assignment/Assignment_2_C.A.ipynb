{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import io\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as map\n",
    "%matplotlib inline\n",
    "import time\n",
    "import panel as pn\n",
    "from panel.interact import interact, fixed\n",
    "\n",
    "# ML import\n",
    "from sklearn import datasets        # datasets\n",
    "from sklearn.cluster import KMeans  # K-Means algorithm\n",
    "from sklearn.cluster import AgglomerativeClustering  # Hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage # dendogram visualization\n",
    "from sklearn.preprocessing import LabelEncoder #Label encoding\n",
    "from sklearn.preprocessing import OneHotEncoder # 1-hot encoding\n",
    "\n",
    "#import from mlxtend for Association rule\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "#Import from surprise library\n",
    "from surprise import Dataset\n",
    "from surprise import get_dataset_dir\n",
    "from surprise import KNNBasic, KNNWithMeans, SVD\n",
    "from surprise.model_selection import train_test_split \n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Welcome to the second assignment! \n",
    "\n",
    "You will have to implement clustering, association rules, and recommender systems algorithms, applying these methods to: \n",
    "- explore the similarities within groups of people watching movies (clustering analysis)\n",
    "- discover the relations between movies genre (association rules)\n",
    "- recommend movies to users (recommender system)\n",
    "\n",
    "We will use the MovieLens dataset, which contains movie ratings collected from the MovieLens website by the [GroupLens](https://grouplens.org/) research lab.\n",
    "\n",
    "Source: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. *ACM Transactions on Interactive Intelligent Systems (TiiS)* 5, 4: 19:1â€“19:19. <https://doi.org/10.1145/2827872>\n",
    "\n",
    "Once you are done you have to submit your notebook here: \n",
    "[https://moodle.epfl.ch/mod/assign/view.php?id=1247726](https://moodle.epfl.ch/mod/assign/view.php?id=1247726)\n",
    "\n",
    "If there is need for further clarifications on the questions, after the assignment is released, we will update this file, so make sure you check the git repository for updates.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ac928-a1e5-4dd7-8e62-b8108c22e5a9",
   "metadata": {},
   "source": [
    "## Clustering analysis: similarities between people (10 points)\n",
    "\n",
    "In this section, you will try to form clusters of individuals based on their preferences regarding movie genres. You will use a transformed version of the MovieLens dataset containing, for a selection of users:\n",
    "- their average rating of all science fiction movies they rated,\n",
    "- their average rating of all comedy movies they rated.\n",
    "\n",
    "Better understanding the differences in people's tastes can help improve the design of recommender systems, for instance for the creation of the user neighborhood. Ok, let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beccfea-24ca-4c3f-92f1-c5a950b2cbce",
   "metadata": {},
   "source": [
    "- Load the data in a dataframe. The url link is provided below. Display the first 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c367091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_dir = r\"C:\\Users\\Charlotte Ahrens\\Documents\\Personal\\GitHub\\MGT-502-Data-Science-and-Machine-Learning\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87e292-3eed-4193-ba24-60693606b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and display data frame\n",
    "url_clustering = 'https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/ratings_clustering.csv'\n",
    "#url_clustering = os.path.join(project_dir,\"ratings_clustering.csv\")\n",
    "df_rating = pd.read_csv(url_clustering)\n",
    "display(df_rating.head(10))\n",
    "display(df_rating.describe())   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f9776b-3f5a-4738-9e27-5d86bc68f6bc",
   "metadata": {},
   "source": [
    "- Plot a Dendrogram using \"ward\" as linkage method and \"euclidean\" as metric. \n",
    "- Based on the Dendrogram, how many clusters do you think is optimal? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebaa0de-e766-42a9-a412-878d417ff40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dendrogram\n",
    "dendo_rating = linkage(df_rating, method=\"ward\", metric=\"euclidean\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "dendrogram(dendo_rating, show_leaf_counts=False)\n",
    "plt.title(\"Dendrogram - individual's movie rating\", fontsize= 16)\n",
    "plt.xlabel(\"Individuals\", fontsize= 13)\n",
    "plt.tick_params(axis = \"x\", labelbottom=False)\n",
    "plt.ylabel(\"Distance\", fontsize= 13)\n",
    "plt.hlines(y= 4.5, xmin=0, xmax=2000, colors= \"k\", linestyle= \"--\")\n",
    "plt.text(y= 4.6, x= 1000, s= \"Horizontal line crossing 4 vertical lines\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be973aee",
   "metadata": {},
   "source": [
    "### Explanation (in green)\n",
    "<font color= darkgreen> \n",
    "\n",
    "\n",
    "Based on the above Dendrogram, I would expect 4 clusters to be the optimal number to cluster the individuals based on their movie genre taste. I determined 4 clusters by looking where I can find the longest vertical line that is not cut by a horizontal line. This distance can be found from ~ y=4 to ~ y=5.8 (Total distance: ~1.8) very close to this are 2 clusters since here we are having a distance from ~ y=5.8 to ~ y=7.2 (Total distance= ~1.4). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783cb37-9721-436d-8915-abbde7c3ee17",
   "metadata": {},
   "source": [
    "- Implement the Elbow method to determine the optimum number of cluster for K-Means algorithm (use `random_state=17` as parameter of K-Means). \n",
    "- Based on the Elbow method, how many clusters do you think is optimal? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bcfd7-156e-4168-a618-94a11ddf2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method for cluster determination \n",
    "inertias = []\n",
    "\n",
    "poss_clusters = range(1,10)\n",
    "for i in poss_clusters:\n",
    "    km = KMeans(n_clusters = i, random_state = 17, n_init=\"auto\")\n",
    "    km.fit(df_rating)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.plot(poss_clusters, inertias, \"-x\")\n",
    "plt.vlines(x= 4, ymin=0, ymax= 120, linestyle= \"--\", colors=\"k\")\n",
    "plt.text(x= 4.2, y= 100, s= \"potential optimum at 5 clusters\")\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow method for inertia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea181feb",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "<font color= darkgreen> \n",
    "\n",
    "\n",
    "The elbow method confirms the previous finding of an optimal of 4 clusters. The Inertia measures how spread out the individuals ratings are within one cluster. The inertia decreases with an increasing # of clusters since the variance within the clusters decreases. Until 4 clusters, the inertia decreases, characterized by a steep slope, while from 4 clusters onwards the slope flattens. The \"elbow\" shows 4 to be the optimal cluster #."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380737c-27d5-4e77-b47f-fe955e0c3e52",
   "metadata": {},
   "source": [
    "- Implement (train) a K-Means algorithm with the number of clusters of your choice. Use `random_state=17` as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c937a-c278-49d6-8af0-c972172864a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means algorithm with random state n = 17\n",
    "kmeans4 = KMeans(n_clusters = 4, random_state = 17, n_init= \"auto\")\n",
    "kmeans4.fit(df_rating)\n",
    "display(kmeans4.cluster_centers_)     \n",
    "display(kmeans4.labels_)                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088087b0-c0d5-4b14-b9f9-db39cf63652c",
   "metadata": {},
   "source": [
    "- Implement (train) a hierarchical algorithm with the same number of clusters as for the K-Means model. Use \"ward\" as linkage method and \"euclidean\" as metric/affinity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc05f2-78a3-4e36-bc9d-a9aef8a83249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering with linkage = ward and metric = euclidean\n",
    "hierarchical4 = AgglomerativeClustering(n_clusters = 4, linkage = \"ward\", metric = \"euclidean\")\n",
    "hierarchical4.fit(df_rating)\n",
    "display(hierarchical4.labels_)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ecb5f-3c7c-469d-b504-98a6c6e8f082",
   "metadata": {},
   "source": [
    "- Create a figure consisting of two subplots:\n",
    "    - a scatterplot of 'avg_scifi_rating' and 'avg_comedy_rating' colored by the clusters predicted with your KMeans model. Add the cluster centers to your plot. Label your clusters with the name of your choice (e.g., \"Comedy aficionado\").\n",
    "    - a scatterplot of 'avg_scifi_rating' and 'avg_comedy_rating' colored by the clusters predicted with your hierarchical algorithm model. Label your clusters with the name of your choice.\n",
    "- How do your models compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eadc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renumber hierarchical data labels so the labels in both algorithm refer to the same labeled clusters \n",
    "dictionary = cluster_relabelling = {2:2, 3:0, 0:3, 1:1}\n",
    "cluster_h = [cluster_relabelling[label] for label in hierarchical4.labels_]\n",
    "\n",
    "# add cluster labels to the original data frame df_rating\n",
    "df_rating[\"cluster_km\"] = kmeans4.labels_\n",
    "df_rating[\"cluster_h\"] = cluster_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b19051-9b33-4ea4-8ef6-0851ee6c13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare kMeans and hierarchical model by creating two subplots\n",
    "fig, ax =  plt.subplots(1,2, figsize= (10,8)) # creating 2 subplots next to another (1 row, 2 columns)\n",
    "fig.suptitle(\"Distribution of individual's movie rating\")\n",
    "lg_labels =[\"Indifferent\",\"Comedy lovers\",\"Imagination geeks\",\"Fun over Imagination\"]\n",
    "\n",
    "#plotting the 1st plot with k_means clusters\n",
    "km_scatter = ax[0].scatter(x= df_rating[\"avg_scifi_rating\"], # x-axis\n",
    "            y= df_rating[\"avg_comedy_rating\"],# y-axis\n",
    "            c = df_rating[\"cluster_km\"],# split data into clusters\n",
    "            cmap = plt.colormaps[\"Set3\"], #color each cluster differently\n",
    "            marker = \"8\") # varies the market style between the clusters\n",
    "ax[0].set_title(\"K-Means with 4 clusters\") # title\n",
    "ax[0].set_xlabel(\"Science fiction\") # x-axis\n",
    "ax[0].set_ylabel(\"Comedy\") # y-axis\n",
    "ax[0].legend(handles=km_scatter.legend_elements()[0], labels=lg_labels)\n",
    "\n",
    "#adding cluster centers\n",
    "ax[0].scatter(kmeans4.cluster_centers_[:,0],\n",
    "              kmeans4.cluster_centers_[:,1],\n",
    "              c= \"black\",\n",
    "              marker = \"X\")\n",
    "\n",
    "\n",
    "#plotting the 2nd plot with hierarchical clusters\n",
    "h_scatter = ax[1].scatter(x= df_rating[\"avg_scifi_rating\"], # x-axis\n",
    "            y= df_rating[\"avg_comedy_rating\"],# y-axis\n",
    "            c = df_rating[\"cluster_h\"], # split data into clusters\n",
    "            cmap = plt.colormaps[\"Set3\"], #color each cluster differently\n",
    "            marker = \"8\") #varies the market style between the clusters\n",
    "ax[1].set_title(\"Hierarchical clustering with 4 clusters\") #title\n",
    "ax[1].set_xlabel(\"Science fiction\") # x-axis\n",
    "ax[1].set_ylabel(\"Comedy\") # y-axis\n",
    "ax[1].legend(labels= lg_labels)\n",
    "ax[1].legend(handles=h_scatter.legend_elements()[0], labels=lg_labels)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e1675d7",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "<font color= darkgreen> \n",
    "\n",
    "Looking at the two graphs above, we see that we are comparing two scatterplots with the same amount of clusters, while the graphs differentiate themselves with the underlying algorithm that computes the individuals into clusters. Starting with the left plot, we observe the cluster \"Indifferent\" and \"Fun over Imagination\" to spark out as they seem to be the clusters with the most observations of all four. On the contrary the cluster \"Imagination geeks\" as well as \"Comedy lovers\" stand out, due to their outliers at the bottom right and top left, and their few observations. In general the main center of observations is captured by \"Indifferent\" at x = 3 andy = 3 and \"Fun over Imagination\" at x=3 and y= 3.5.\n",
    "\n",
    "Comparing this with the hierarchical clustering we find a more equal distribution of the center between \"Indifferent\", \"Fun over Imagination\", and \"Imagination geeks\". Especially \"Indifferent\" lost many of its observations to the grey and the yellow colored clusters and is therefore comparably small. Also the blue colored cluster \"Comedy lovers\" lost some of its already few observations, mainly to the yellow cluster. \n",
    "\n",
    "The differences in cluster size and distribution is seen due to the different algorithms that we apply. The k_means model, is an iterative model that starts with the given number of cluster centers and assigns each data point to the nearest center. It then updates the centers to the mean of the data points assigned to them, repeating this process until teh centers are not moving anymore. While the hierarchical /agglomerative clustering doesn't require a given center number (we determined this one through the elbow method) and it is considering each data point as a separate cluster and then merges clusters based on their similarity. To mention is that in both cases the algorithm tries to reduce the euclidean distance between the data points, while additionally the hierarchical cluster centers are calculated by the ward linkage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195ca1f-77d2-446f-a178-8665c213aef1",
   "metadata": {},
   "source": [
    "## Association Rules: association between movie genres (10 points)\n",
    "\n",
    "You will now pursue your analysis, but this time trying to dig out information about movies. More precisely, you will search for matches between film genres using association rules. We try to understand, for instance, how likely it is that a film is both drama and action. This information can be interesting for film producers who may either want to produce something similar to the established norm: if most drama films are also action, perhaps the new action-drama film would be equally appreciated, or quite to the contrary try a new combination of genres which is more rare to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb45c9e-c071-4dad-8011-445ff946d797",
   "metadata": {},
   "source": [
    "- Load the data in a dataframe. The url link is provided below. \n",
    "- Display the first 10 observations. \n",
    "- Print the unique values of genres from the first column. \n",
    "- How many unique genres does the first column contain? \n",
    "- How many movies does the dataframe contains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764800a3-1399-480f-abb3-d79609193866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and display movie genre dataframe\n",
    "url_association_rules = 'https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/movies_assoc_rules.csv'\n",
    "#url_association_rules = os.path.join(project_dir,\"movies_assoc_rules.csv\")\n",
    "df_movies = pd.read_csv(url_association_rules)\n",
    "display(df_movies.head(10))\n",
    "display(df_movies.describe())\n",
    "\n",
    "#print the unique genres\n",
    "print(df_movies[\"0\"].unique(),'\\n')\n",
    "print(\"The first column of the genre dataframe contains %d different genres.\" % len(df_movies[\"0\"].unique()))\n",
    "\n",
    "#double check that all genres were displayed in the first column.\n",
    "# convert the movie data frame to a list \n",
    "movie_list = df_movies.values.tolist()\n",
    "\n",
    "# delete all the nan'sets and just key genres in the list\n",
    "movie_list_cleaned = [[x for x in row if str(x) != \"nan\" ] for row in movie_list] # if my dataset would be numerical I could use: not math.isnan(x)\n",
    "total_movies_list = [x for y in movie_list_cleaned for x in y]\n",
    "\n",
    "#display the individual lists of above in one big list to find unique values:\n",
    "total_movies = set(total_movies_list)\n",
    "print(\"The total number of different genres in the dataframe is %d.\" % len(total_movies))\n",
    "\n",
    "#Total number of movies\n",
    "non_nan = df_movies[\"0\"].count().sum()\n",
    "print(\"The dataframe contains in total %d movies.\" %non_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd9f12-a4a8-46ba-8cdb-ef0a3c9bf6c8",
   "metadata": {},
   "source": [
    "- Preprocessing: as seen during the lab, convert the dataset using a `Transaction Encoder` from the `mlextend` module so that the dataset is reorganised in columns of unique genres. Rows should contain only True or False boolean values according to whether a film was considered as belonging to a genre column or not. Check that you have the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16ef7d-81a3-4fbb-95bd-88f5a2d31302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the encoder\n",
    "te = TransactionEncoder()\n",
    "\n",
    "#fit and transform the data\n",
    "movie_list_encoded= te.fit(movie_list_cleaned).transform(movie_list_cleaned)\n",
    "\n",
    "#create dataframe with type of genre as header\n",
    "df_movie_encoded = pd.DataFrame(movie_list_encoded, columns=te.columns_)\n",
    "display(df_movie_encoded.head(10))\n",
    "print(\"The dataframe contains %d observations and %d different genres.\" %(len(df_movie_encoded[0:]),df_movie_encoded[:1].sum().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e0e99-7012-4efa-99e4-56eb9bd5eee5",
   "metadata": {},
   "source": [
    "- Frequent itemsets: using the Apriori algorithm to find the frequent itemsets with minimum support of 0.01. There is no condition on the maximum length of an itemset. \n",
    "- How many itemsets did the apriori algorithm return above (for min_support=0.01)? \n",
    "- What are the 10 itemsets with the largest support (you can directly display a dataframe with the 10 itemsets and their support)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fe963-f30a-452c-9a64-9d74c3b8431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine frequent itemsets with min support 0.01 and no max_len\n",
    "freq_items = apriori(df_movie_encoded, min_support= 0.01, use_colnames= True)\n",
    "display(freq_items.sort_values(by=\"support\",ascending=False).head(10))\n",
    "display(\"With a minimum support of 0.01, the Apriori algorithm returned %d itemsets.\" %len(freq_items[\"support\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e5245e6",
   "metadata": {},
   "source": [
    "The itemseets with the largest support are Drama with a support on fearly 0.5. It is followed by Comedy with a support of 0.31. After, thriller and romance are following with a very similar support at ~0.15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e3247-3a63-4f7a-b43a-16c59b95708a",
   "metadata": {},
   "source": [
    "- Mining for association rules: using the frequent items identified above, find association rules with a minimum confidence of 0.45 and order them by decreasing value of lift.\n",
    "- Discuss the following statements (true or false with 1-2 lines justification)\n",
    "    - Animation films are associated with Children.  \n",
    "    - If a film has the genre Musical, then it is also a Comedy.\n",
    "    - If War then Drama is the asociation rule with the highest confidence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff18aa-cda1-4402-857f-1b31c95ddecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine for association rules with confidence >= 0.45\n",
    "\n",
    "rules_genre = association_rules(freq_items, metric = \"confidence\", min_threshold = 0.45)\n",
    "rules_genre = rules_genre.sort_values(by = \"lift\", ascending=False)\n",
    "display(rules_genre)\n",
    "Musical_relations = rules_genre[rules_genre[\"antecedents\"].astype(str).str.contains(\"Musical\")]\n",
    "Musical_relations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b53625e8-c4f0-4146-a4be-2c03f8df1b2a",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "<font color= darkgreen> \n",
    "\n",
    "1. Animation films are associated with Children. True. \n",
    "\n",
    "Due to its highest lift in this dataset with ~10.9. Lift measures the strength of the association between the antecedent and the consequent and therefore the above rule is valid, that when we have Animation movies, the likelihood of it being also a children genre is very high. \n",
    "\n",
    "2. If a film has the genre Musical, then it is also a Comedy. False.\n",
    "\n",
    " There is an association, meaning a relationship between these two genres, but not a equalization or implication. In addition the rule is quite weak with a lift of 1.32 and therefore even a relationship is not strong between Music and Comedy.\n",
    "\n",
    "3. If War then Drama is the association rule with the highest confidence. True.\n",
    "\n",
    "If war is the antecedent, then the consequent \"Drama\" has a confidence of ~0.749. In this list this is the highest confidence, followed by \"if Romance, then Drama\" with a confidence of 0.623.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08235a6-27ec-45b7-9adc-0eea440b6b02",
   "metadata": {},
   "source": [
    "## Recommender systems: item-based recommender system (10 points)\n",
    "\n",
    "In the walkthrough, we have implemented a user-to-user collaborative filtering algorithm (from scratch and using using Surprise library), i.e., our recommendations were based on the ratings of users with similar tastes. In this assignment, you will implement an **item-to-item** collaborative filtering algorithm, i.e., the recommendations will be based on the set of movies that users like. Do not worry, you won't have to implement the algorithm from scratch and instead can rely on the [Surprise library](http://surpriselib.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc548b9a-1a50-4ada-beb4-100ffe9bd701",
   "metadata": {},
   "source": [
    "- As in the walkthrough, load the *built-in* `ml-100k` from the Surprise library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678f71a-015e-4fd6-aae9-ec15ccc47b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the builtin dataset\n",
    "data_recomm = Dataset.load_builtin(\"ml-100k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b550a4-6dd1-441a-8d55-ee8b3113a932",
   "metadata": {},
   "source": [
    "- Use GridSearchCV to find the best number of neighbors (k) for a KNNWithMeans **item-based** algorithm, with the following parameters:\n",
    "    - options for k: `[10, 20, 30, 40, 50]`\n",
    "    - `'sim_options': {'name': ['pearson'], 'user_based': [???]}` Here you have to replace `???` with the appropriate value...\n",
    "    - root-mean-square-error (RMSE) as measures,\n",
    "    - 5 cross-validation folds,\n",
    "    - other parameters: `refit=True, joblib_verbose=2, n_jobs=-1`\n",
    "- What is the optimal k for which GridSearchCV returned the best RMSE score? \n",
    "- What is the RMSE score for the optimal k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3971dda-842f-4091-9afe-cb186943e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the parameters for item based filtering \n",
    "sim_k_grid = {\"k\": [10,20,30,40,50],\n",
    "              \"sim_options\": {'name': ['pearson'], 'user_based': [False]}} # item based collaborative filtering\n",
    "                            \n",
    "\n",
    "#build a recommender system with GridSearchCV and fit the model\n",
    "knn_cv = GridSearchCV(KNNWithMeans, sim_k_grid, measures = [\"rmse\"],cv=5, refit=True, joblib_verbose=2, n_jobs=-1)\n",
    "knn_cv.fit(data_recomm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print results: \n",
    "print(\"Optimal k neighbour:\", knn_cv.best_params[\"rmse\"][\"k\"])\n",
    "print(\"RMSE score for optimal k: \", round(knn_cv.best_score[\"rmse\"],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd98fcb-c389-4807-b50b-bf8d56e5404b",
   "metadata": {},
   "source": [
    "- Using the Surprise library, split your dataset between training and test set. As parameters, use `test_size=0.2, random_state=12`\n",
    "- Fit a KNNWithMeans algorithm using the best k value retrieved above. As other parameters, use:\n",
    "    - `min_k=1`\n",
    "    - `sim_options = {'name': 'pearson','user_based': ???}`\n",
    "    - `verbose=False`\n",
    "- Predict ratings on the test set using your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342582d-44a9-486b-bcf5-6a58000487a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a training and test set\n",
    "recomm_train,recomm_test = train_test_split(data_recomm, test_size=0.2, random_state=12)\n",
    "\n",
    "#create KnnMeans algorithm and fit train data\n",
    "knn_means = KNNBasic(k= 50, min_k=1,sim_options= {'name': 'pearson','user_based': False},verbose=False)\n",
    "knn_means.fit(recomm_train)\n",
    "\n",
    "#predict ratings\n",
    "predictions = knn_means.test(recomm_test)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250babc-8ba9-4234-9d21-429766d34bd4",
   "metadata": {},
   "source": [
    "- Use the helper function below to identify the best 10 films for all users\n",
    "- Find the top 10 predictions for user 169 (you should return the titles of the movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe1156-6120-416a-a03e-4b126d2964f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_item_names():\n",
    "    '''Read the u.item file from MovieLens 100-k dataset and return two\n",
    "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
    "    '''\n",
    "\n",
    "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list) # This is used to group a sequence of key-value pairs into a dictionary of lists\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f88a04-9eb7-49da-90ea-0f914124cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying the best 10 films for all users\n",
    "#display(read_item_names())\n",
    "\n",
    "# get user ID's in a list\n",
    "top_10 = get_top_n(predictions, n=10)\n",
    "user_id = list(top_10.keys())\n",
    "#display(user_id)\n",
    "\n",
    "# get user movie names in a list\n",
    "movie_names = list(top_10.values())\n",
    "#display(movie_names)\n",
    "\n",
    "# applying the helper function\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "# create list for top 10 movies of each user\n",
    "top_10_per_user = []\n",
    "\n",
    "for i in user_id: \n",
    "    top_10_user = [i]\n",
    "    for j in range(len(top_10[i])):\n",
    "        movie_names = [rid_to_name[top_10[i][j][0]]],\n",
    "        rating_user = round(top_10[i][j][1],2)\n",
    "      #  top_ten_per_user.append(movie_names)\n",
    "        top_10_user.append(rid_to_name[top_10[i][j][0]]), round(top_10[i][j][1],3)\n",
    "    top_10_per_user.append(top_10_user)\n",
    "\n",
    "#show top 10 in Panda Dataframe\n",
    "df_top_10_per_user = pd.DataFrame(top_10_per_user)\n",
    "df_top_10_per_user =  df_top_10_per_user.rename(columns={0: \"user_ID\"})\n",
    "display(df_top_10_per_user.sort_values(\"user_ID\").reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417dbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 film predictions for user 169\n",
    "user_id = \"169\"\n",
    "user_rating = top_10[user_id]\n",
    "#sort descending - the movie with the highest rating on the top\n",
    "sorted_user_rating = sorted(user_rating, key=lambda x: x[1],reverse=True)\n",
    "for i in sorted_user_rating:\n",
    "    print (rid_to_name[i[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0093ba6-a590-4969-bd78-5fa06b4f51b5",
   "metadata": {},
   "source": [
    "- Plot the precision at rank k and the recall at rank k on the same figure, for k between 0 and 20, and a relevance threshold of 3.75\n",
    "- Plot the precision-recall curve\n",
    "\n",
    "*You can, but do not have to, rely on the function(s) used in the lab (i.e., copying the code of the function(s))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4c671-baa8-49e3-8dc4-b86add32b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get function for precision and recall of my algorithm\n",
    "def precision_recall_algo(algo):\n",
    "    '''Return precision and recall at k metrics for an algorithm.'''    \n",
    "    \n",
    "    # Fit algo on training set\n",
    "    algo.fit(recomm_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    predictions = algo.test(recomm_test)\n",
    "    \n",
    "    # Compute precision and recall\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for k in range(21):\n",
    "        precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.75)\n",
    "        precision.append( sum(prec for prec in precisions.values()) / len(precisions) )\n",
    "        recall.append( sum(rec for rec in recalls.values()) / len(recalls) ) \n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "#get function for precision and recall for each user\n",
    "def precision_recall_at_k(predictions, k=50, threshold=3.75):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply precision and recall function \n",
    "precision_val, recall_val = precision_recall_algo(knn_means)\n",
    "\n",
    "# Plot the graph for recall and precision\n",
    "plt.plot(range(21), recall_val, \"green\", label=\"Recall\", marker = \"8\")\n",
    "plt.plot(range(21), precision_val, 'red', label=\"Precision\",marker = \"8\")\n",
    "plt.legend()\n",
    "plt.title(\"Precision and recall for item-based KNN\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.xticks(np.arange(0, 21, step=1))\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef45eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN precision and recall graph\n",
    "#apply precision and recall function \n",
    "precision_val, recall_val = precision_recall_algo(knn_means)\n",
    "\n",
    "# Plot\n",
    "plt.step(recall_val, precision_val, color='g', where='post', label ='KNN (with k=50 and item-based)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.title('Precision-Recall curve');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbc3b538-551c-4098-b82f-8c00d976a448",
   "metadata": {},
   "source": [
    "Congrats, you are done with the assignment! YAY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f99bcdbb3217576de7d4913388132c7a29c76b283dfc7cc14da5cb2abf39e8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
